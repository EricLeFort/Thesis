\chapter{Conclusions and Future Work}
As can be noted from the results in the previous section, this research leads to some definitive conclusions involving both the efficacy of these machine learning techniques as applied to this specific problem as well as how the models relate to one another in terms of their training and execution time.

In terms of accuracy a couple conclusions can be made. First, naive bayes, artificial neural networks, and logistic regression did not perform well, they did not even surpass the baseline accuracy. On the other hand, SVM, k-NN, and boosting all performed quite well (other than SVM's tendency to only predict within a +/- 3\% range). These conclusions can be useful for selecting which algorithms to prioritize experimentation with for problems which are similar in nature to this one.

In terms of speed a couple conclusions can be made. If execution speed is of concern for a given problem, the best candidates are logistic regression, decision trees, naive bayes, or small neural networks. Some examples where this may be the case include Intrusion Detection Systems (IDSs), autonomous robotics such as self-driving cars, or system controllers to name a few. If training speed is of concern for a given problem, the best candidates are naive bayes or k-NN. Training speed will typically only be a major concern if the application deals with active learning. However, if a dataset is extremely large, the cost in development time may also become a concern. For the problem considered in this paper, execution time is not a concern. Therefore the best model for the job is simply the one which reliably performed the best and also displays the ability to adapt to change.

By simply going by the numbers, SVM would be selected since it predicted the number of applicants most accurately. However, looking at the prediction histograms shows that this model was hardly modifying its prediction from applicant to applicant. Therefore, if a new year's dataset had a different proportion of acceptances, this model's prediction would likely suffer. Since k-NN did not share this prediction behaviour and still performed well, it should be the one selected as the best model for this task.

One of the most surprising results was that arguably the most elementary of the models, plain decision trees, performed so amicably. As always, the ``no free lunch'' theorem remains valid. Perhaps with more studies of this nature performed, the scientific community will one day be able to discover a technique which reliably predicts which algorithm best suits a given problem. However, until that day comes, it is important not to eliminate algorithmic alternatives without first testing their efficacy for that specific problem.